{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02aa9a84",
   "metadata": {},
   "source": [
    "# Interpretability of the Image Quality Metrics (IQMs) of MRIQC\n",
    "\n",
    "[MRIQC](https://mriqc.readthedocs.io/en/latest/) is a powerful tool to assess the quality of MR images in a research study. \n",
    "In addition to a visual report, a number of image quality metrics (IQMs) are generated. \n",
    "However, there is a large number of these metrics and it is not immediately obvious which IQM a researcher should pay most attention to when deciding over the quality of a given image.\n",
    "\n",
    "In this notebook, we will explore these issues in the MR-ART dataset, to provide researchers guidance in interpreting and selecting the most important IQMs from MRIQC. \n",
    "If you want to follow along and run the notebook yourself, please download the runnable notebook at [https://github.com/brainhack-ch/interpret-iqms/blob/main/code/interpretability-of-iqms.ipynb](https://github.com/brainhack-ch/interpret-iqms/blob/main/code/interpretability-of-iqms.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fd270b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c45a8bd",
   "metadata": {},
   "source": [
    "First, let's import the data. The [Movement-Related Artifacts (MR-ART)](https://openneuro.org/datasets/ds004173) dataset contains the T1-weighted images of 148 healthy subjects (Nárai et al. 2022). \n",
    "Each subject has been acquired under three motion conditions:\n",
    "\n",
    "1. no head movement\n",
    "2. little head movement\n",
    "3. much head movement\n",
    "\n",
    "The motion was artifically induced by giving the subjects cues when to node their head.\n",
    "\n",
    "The images were rated by to two expert neuroradiologists, who rated the images in their quality, with ratings\n",
    "\n",
    "1. good quality\n",
    "2. medium quality\n",
    "3. bad quality.\n",
    "\n",
    "The neuroradiologist were asked to harmonize their ratings, hence the dataset contains only one score per images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to the data\n",
    "path_data = os.path.abspath(\"iqms_interpretability_files/\")\n",
    "# import IQMs\n",
    "iqms = pd.read_csv(os.path.join(path_data, \"IQMs.tsv\"), index_col=\"bids_name\", sep=\"\\t\")\n",
    "# import rating scores\n",
    "scores = pd.read_csv(os.path.join(path_data, \"scores.tsv\"), index_col=\"bids_name\", sep=\"\\t\")\n",
    "# make sure they are in the same order\n",
    "iqms.sort_index(inplace=True)\n",
    "scores.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e5e25",
   "metadata": {},
   "source": [
    "Let's combine the motion condition and the manual ratings in one dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_condition_column(scores):\n",
    "    \"\"\"Adds a column with the different movement conditions to the rating scores dataframe\"\"\"\n",
    "    stand = scores.index.str.contains(\"standard\")\n",
    "    hm1 = scores.index.str.contains(\"motion1\")\n",
    "    hm2 = scores.index.str.contains(\"motion2\")\n",
    "    conditions = [\n",
    "        (stand == True),\n",
    "        (hm1 == True),\n",
    "        (hm2 == True)]\n",
    "    choices = [1, 2, 3]\n",
    "    scores['condition'] = np.select(conditions, choices)\n",
    "    return scores\n",
    "scores = add_condition_column(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0d488",
   "metadata": {},
   "source": [
    "## Rating scores and image conditions\n",
    "\n",
    "We can explore how well the raters align in their scores with the motion condition. \n",
    "Does their rating reflect how much people moved in the scanner? \n",
    "Let's plot the confusion matrix of the scores assigned by the rater and the motion condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d587d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(pd.crosstab(scores[\"score\"], scores[\"condition\"]), annot=True, fmt=\".0f\")\n",
    "sns.set(font_scale=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fdfcf7",
   "metadata": {},
   "source": [
    "We can see that generally there is good correspondance, with most values being on the main diagonal. Most confusion is for score 2, where there are also a lot of images from the no-motion as well as the much motion conditions.\n",
    "\n",
    "Some images are however completely misclassified. We can visualize this using the library [Plotly](https://plotly.com/python/), which allows for creating a variety of interactive plots. With this, it will be easier to identify the images that were rated much better or much worse than the condition they were in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.strip(scores, x=\"condition\", y=\"score\", color=scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ebcda",
   "metadata": {},
   "source": [
    "The graphing library `plotly` allows to color the points according to the name of the image, such that we can identify the outliers by hovering over them. We can see that image `sub-613957_acq-standard_T1W` got a bad rating, even though this was the image from the no-motion condition. \n",
    "\n",
    "For the three images `sub-253493_acq-headmotion2_T1w`, `sub-257563_acq-headmotion2_T1w`, and `sub-567742_acq-headmotion2_T1w` we have the opposite case: these images got the best rating, even though the subjects were supposed to move their heads a lot during the acquisition of these images.\n",
    "\n",
    "Try and look at the HTML reports of these images: can you guess what was likely going on in each of these cases?\n",
    "\n",
    "## Visualizing the IQMs\n",
    "\n",
    "Next, let's look at the IQMs. There are many different variables here:\n",
    "\n",
    "### Preprocessing the IQMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqms.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdd0ea",
   "metadata": {},
   "source": [
    "Some of them we can get rid of right away, as they are not image quality metrics, but measurement parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dfad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqms_use = iqms.drop([\"size_x\", \"size_y\", \"size_z\", \"spacing_x\", \"spacing_y\", \"spacing_z\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9e81c",
   "metadata": {},
   "source": [
    "We should also normalize the data, as the units of the IQMs vary wildly. \n",
    "This is required by some methods like PCA down the line, but it also makes sense computationally to have all values in the same order of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "iqms_scaled = scaler.fit_transform(iqms_use)\n",
    "iqms_scaled = pd.DataFrame(iqms_scaled, columns=iqms_use.columns, index=iqms_use.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3aa021",
   "metadata": {},
   "source": [
    "Now that we preprocessed our IQMs, let us combine the IQMs, the manual ratings and the motion condition in one dataframe to more easily use it in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96acee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We merge the dataframes based on their indexes, which corresponds to the bids name of each scan\n",
    "data_df = pd.merge(left=scores, left_index=True, right=iqms_scaled, right_index=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a633822",
   "metadata": {},
   "source": [
    "### Visualizing the data (always!)\n",
    "\n",
    "Rule number one of data analysis: always visualize your data! \n",
    "We can plot the pairwise scatterplots to get an idea of the relationships between the IQMs. \n",
    "There are quite a few IQMs, so plottings this might take a few minutes, depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bbebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sets = {f\"set{i+1}\": list(iqms_scaled.columns[i*10:i*10+9]) for i in range(6)}\n",
    "metrics_sets[\"set8\"] = list(iqms_scaled.columns[60:])\n",
    "for metrics_set in metrics_sets.values():\n",
    "     sns.pairplot(iqms_scaled[metrics_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce25a6d",
   "metadata": {},
   "source": [
    "Interesting! \n",
    "There are quite a few non-linear relationships between the IQMs, which we should keep in mind should linear dimension reduction techniques fail. \n",
    "Also note that some metrics are heavily correlated which renders IQMs the perfect candidates for dimensionality reduction. \n",
    "Dimensionality reduction would also help us in our quest to interpret the IQMs by providing a lower dimensional representation. \n",
    "Let's try.\n",
    "\n",
    "## Dimensionality Reduction\n",
    "\n",
    "### PCA\n",
    "\n",
    "The simplest dimension reduction technique is principal component analysis (PCA), which projects the data onto a new set of axis defined by the eigenvectors of the data matrix (principal components). \n",
    "The first principal component corresponds to the direction in which the variance is maximal. \n",
    "For PCA, it is important for all the features to be on the same scale, thus data need to be normalized. \n",
    "Here, as the data are already rescaled, we can directly apply it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA\n",
    "pca = PCA()\n",
    "iqms_pca = pca.fit_transform(iqms_scaled)\n",
    "\n",
    "#Caste PCA results as a dataframe\n",
    "col_names_pca = [f\"component{i+1}\" for i in range(62)]\n",
    "iqms_pca = pd.DataFrame(iqms_pca, columns=col_names_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd42899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the two first principal components color-coded by motion condition\n",
    "fig = px.scatter(iqms_pca, x=\"component1\", y=\"component2\", color=scores[\"condition\"],\n",
    " title=\"Projection of the data colored by motion condition on the two first principal components\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the two first principal components color-coded by manual ratings\n",
    "fig = px.scatter(iqms_pca, x=\"component1\", y=\"component2\", color=scores[\"score\"],\n",
    "title=\"Projection of the data colored by manual ratings on the two first principal components\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb5cd28",
   "metadata": {},
   "source": [
    "We can see that the projection of the data on the two first components already allows us to separate well both the ratings and the conditions. \n",
    "Let's see how much variance those two components explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd314911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract variance explained by each component\n",
    "exp_var=pca.explained_variance_ratio_\n",
    "#Compute cumulative variance explained\n",
    "cum_exp_var = np.cumsum(exp_var)\n",
    "\n",
    "#Plot cumulative variance explained\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(cum_exp_var, 'bo-')\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel('Component index',fontsize=16)\n",
    "plt.ylabel('Cumulative variance explained',fontsize=16)\n",
    "\n",
    "print('The two first principal components explain {:.0f}% of the variance.'.format(cum_exp_var[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baacd077",
   "metadata": {},
   "source": [
    "The two first principal components explain 62% of the variance. \n",
    "If the goal was to do a dimensionality reduction that retains as much variance as possible, we would want to extract more than two components. \n",
    "However, we already saw that the first component is able to separate the different conditions and ratings already quite well. \n",
    "So for exploration purposes, it makes sense to have a look at the loadings of just the first two components. \n",
    "This way, we will get a sense of which variables contribute most to each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(pca.components_[:2].T, index=iqms_scaled.columns)\n",
    "px.imshow(loadings, color_continuous_midpoint=0, color_continuous_scale=px.colors.diverging.RdBu_r,\n",
    "width=500, height=1300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b78a74c",
   "metadata": {},
   "source": [
    "As we can see do the two components capture different information. \n",
    "We already know from the scatterplot that the first component is the one able to separate the conditions/ratings. \n",
    "When looking at the loadings of the first component, we get an idea which of the IQMs influence this component the most. \n",
    "[Here](https://mriqc.readthedocs.io/en/latest/iqms/t1w.html) is an overview of the different measures, that also tells us if higher or lower values indicate a better quality image. \n",
    "E.g., for CJV, lower is better, while for CNR, higher is better. \n",
    "This means that component 1 is related to the \"badness\" of the image, as the polarity of the loadings of CJV and CNR is opposite to the interpretation of their value.\n",
    "\n",
    "### t-SNE\n",
    "\n",
    "T-distributed Stochastic Neighbor Embedding (t-SNE) is another dimension reduction technique. \n",
    "Unlike PCA, it also captures non-linear relationships. \n",
    "It is often used for visualization purposes. \n",
    "Let's run t-SNE to get another visualization of the data in latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cc5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run TSNE\n",
    "iqms_embedded = TSNE(n_components=2, learning_rate='auto', init='random' , perplexity=3, random_state=42).fit_transform(iqms_scaled)\n",
    "# plot TSNE\n",
    "iqms_embedded = pd.DataFrame(iqms_embedded, columns=[\"component1\", \"component2\"])\n",
    "df = pd.DataFrame()\n",
    "df[\"score\"] = scores[\"score\"].copy()\n",
    "df[\"condition\"] = scores[\"condition\"].copy()\n",
    "\n",
    "df[\"comp-1\"] = iqms_embedded[\"component1\"].values\n",
    "df[\"comp-2\"] = iqms_embedded[\"component2\"].values\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=\"score\",\n",
    "                palette=sns.color_palette(\"hls\", 3),\n",
    "                data=df, ax=axs[0]).set(title=\"IQMs T-SNE projection, colored by rating score\")\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=\"condition\",\n",
    "                palette=sns.color_palette(\"hls\", 3),\n",
    "                data=df, ax=axs[1]).set(title=\"IQMs T-SNE projection, colored by motion condition\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8873471",
   "metadata": {},
   "source": [
    "While different from the PCA components, the separation does not seem to improve. \n",
    "If anything, it is worse, as now both components are needed to separate images with rating score 1 from those with 2 or 3, instead of just one component when using PCA. \n",
    "The same is true when looking at the movement condition.\n",
    "\n",
    "### Conclusion about dimensionality reduction\n",
    "\n",
    "We saw using TSNE and PCA that it is possible to separate the IQMs in a lower-dimensional space based on the motion condition or the manual ratings.\n",
    "However, that did not help us interpret the IQMs. We checked the loadings of the principal components, but it is difficult to extract a clear story. \n",
    "Let's try another approach. \n",
    "\n",
    "## Classification and Feature Importance\n",
    "\n",
    "While dimension reduction in general and PCA in particular can give us an idea of which IQMs are useful in telling good from bad quality images, a more direct approach is to use the IQMs as features in a classification task. \n",
    "We will try two supervised machine learning algorithm, namely a logistic regression with elastic-net regularization and support vector classification (SVC).\n",
    "\n",
    "To make the interpretation of the plots easier, we will moreover binarize our target: no-motion versus motion, meaning that we pulled together in one group both levels of motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"condition_bin\"] = np.where(scores[\"condition\"] == 1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21629983",
   "metadata": {},
   "source": [
    "### Classification approach\n",
    "\n",
    "To implement SVC and Elastic-net classifications, we will follow the following steps:\n",
    " * define a final testing set on which to assess the performance of the classifier with selected hyper-parameters\n",
    " * do cross-validation on the remaining data to examine the stability of the best hyper-parameters chosen for each test fold\n",
    " \n",
    "The final test set will be 10% the size of the data, and the cross-validation scheme will be stratified shuffled 10-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709aba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.1\n",
    "n_folds = 10\n",
    "\n",
    "X = iqms_scaled\n",
    "y = scores[\"condition_bin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffaa211",
   "metadata": {},
   "source": [
    "**Data split**. \n",
    "The first step of any machine learning method is to split the dataset in a train and a test set. \n",
    "The train set will be used to train the model in a cross-validate fashion. \n",
    "The test set will be used to assess the model accuracy on a set that has not been seen by the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf69425",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X.to_numpy()\n",
    "y_np = y.to_numpy()\n",
    "X_cv, X_finaltest, y_cv, y_finaltest = train_test_split(X_np, y_np, test_size=test_ratio, shuffle=True,\n",
    "                                                          stratify=y_np, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb8722e",
   "metadata": {},
   "source": [
    "**Cross-validation.** \n",
    "Next, a stratified shuffled 10-fold cross-validation is performed to find the set of best hyperparameters. \n",
    "Stratified means that the folds are made by preserving the percentage of samples for each class. \n",
    "The hyperparameters set selected is the one providing the best performance in the test folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabfc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9668fd7",
   "metadata": {},
   "source": [
    "### Classification using Elastic-Net\n",
    "\n",
    "We first try to predict the motion condition based on the IQMs using Elastic-Net (Zou and Hastie 2005) logistic regression. \n",
    "It combines L1 and L2 regularization, thus both promoting sparsity of the solution -- which is an advantage for feature selection -- and accounting for feature collinearity.\n",
    "\n",
    "#### Training the model\n",
    "\n",
    "We will do a grid search to find the best parameters for our learning algorithm. \n",
    "In this case, these will be the C, which referes to the regularization strength, and the L1 ratio, which is the ratio of the L1 penalty to the L2 penalty (remember that elastic net combines both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1822e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [ 10.0**p for p in np.linspace(-6, 6, 100)]\n",
    "l1_ratios = np.linspace(0, 1.0, 41)\n",
    "n_cpus=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model\n",
    "en_cv = LogisticRegressionCV(max_iter=1000000, penalty='elasticnet', solver=\"saga\", class_weight=\"balanced\",\n",
    "                             cv=skf, random_state=42, l1_ratios=l1_ratios, Cs=Cs, n_jobs=n_cpus)\n",
    "                             \n",
    "#Fit the model to the data using the target\n",
    "en_cv.fit(X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best set of paramaters are l1 ratio: {en_cv.l1_ratio_[0]} and C: {en_cv.C_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a788540",
   "metadata": {},
   "source": [
    "#### Testing the model\n",
    "\n",
    "To estimate the final model accuracy, we test the model on the hold-out test data and compare it to training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae09925",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_best = LogisticRegression(max_iter=1000000, penalty='elasticnet', solver=\"saga\", class_weight=\"balanced\",\n",
    "                             l1_ratio=1.0, C=en_cv.C_[0])\n",
    "en_best.fit(X_cv, y_cv)\n",
    "print(f\"Training accuracy: {accuracy_score(y_cv, en_best.predict(X_cv))}\")\n",
    "print(f\"Hold out dataset test accuracy: {accuracy_score(y_finaltest, en_best.predict(X_finaltest))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46212bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_finaltest, en_best.predict(X_finaltest), labels=en_best.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=en_best.classes_)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104cc1c",
   "metadata": {},
   "source": [
    "#### Visualizing model coefficients\n",
    "\n",
    "Inspecting the model coefficients can tell us about what features are important for the model to make its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f4ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel_coefs_df = pd.DataFrame({\"coef\": en_best.coef_[0]})\n",
    "bestmodel_coefs_df[\"abs_coef\"] = bestmodel_coefs_df[\"coef\"].abs()\n",
    "bestmodel_coefs_df[\"coef_name\"] = iqms_scaled.columns\n",
    "bestmodel_coefs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd8f0e",
   "metadata": {},
   "source": [
    "Non-surprinsingly, many coefficients are zero due to the high value of L1 ratio chosen for the best model corresponding to a model favoring L1 penalty. Let's have a look only at the non-zero median features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdf301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the coefficients by absolute value and plot them\n",
    "coefs_sorted_df = bestmodel_coefs_df.sort_values(by='abs_coef',ascending=False)\n",
    "# Select only non-zero coefficients\n",
    "coefs_sorted_non0_df = coefs_sorted_df[coefs_sorted_df[\"abs_coef\"]>0]\n",
    "# Stem plot\n",
    "plt.figure(figsize=(13,6))\n",
    "plt.stem(\"coef\", data=coefs_sorted_non0_df[coefs_sorted_non0_df[\"abs_coef\"]>0])\n",
    "plt.xticks(np.arange(len(coefs_sorted_non0_df)), coefs_sorted_non0_df[\"coef_name\"], rotation='vertical');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08afcd",
   "metadata": {},
   "source": [
    "We can also visualize them in a polar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6aea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "# Do not display warnings\n",
    "\n",
    "# Visualization in form of polar plot\n",
    "coefs_sorted_non0_df[\"positive\"] = coefs_sorted_non0_df[\"coef\"] > 0\n",
    "px.line_polar(coefs_sorted_non0_df, r='abs_coef', theta='coef_name', color='positive', line_close=True, \n",
    "              hover_data={\"coef\": ':.3f', \"coef_name\": True, \"positive\": False, \"abs_coef\": False},\n",
    "              width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c022e3",
   "metadata": {},
   "source": [
    "As a conclusion, the most important features to predict motion presence based on the IQMs are the SNR-derived metrics along with the white matter to maximum intensity ratio (wm2max) and a few summary metrics such as the kurtosis of the background (summary_bg_k) or the number of voxels in the white matter (summary_wm_n). \n",
    "Remember that you can find the list of all the IQMs and their definition [here](https://mriqc.readthedocs.io/en/latest/measures.html).\n",
    "\n",
    "### Classification using Support vector classifier (SVC)\n",
    "\n",
    "To see if these results also hold when using a different classifier, we follow the same procedure using a Support Vector Machine (SVC).\n",
    "\n",
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81cc38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7a099",
   "metadata": {},
   "source": [
    "Again, we do a grid search over the parameters in a cross-validated fashion. This time there is just one parameter, the regularization strength (parameter C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e080a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [ 10.0**p for p in np.linspace(-6, 6, 100)]\n",
    "n_cpus=20\n",
    "param_grid = {\"C\": Cs}\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, n_jobs=n_cpus, cv=skf)\n",
    "grid_search.fit(X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61af85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best training accuracy reached in the grid search is: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12622a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best hyperparamater C is: {grid_search.best_params_['C']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fe536",
   "metadata": {},
   "source": [
    "#### Testing the model\n",
    "\n",
    "To estimate the final prediction accuracy, we again test the model on the hold-out test data and compare it to training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3625da",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best = LinearSVC(class_weight=\"balanced\", max_iter=100000, C=grid_search.best_params_['C'])\n",
    "svc_best.fit(X_cv, y_cv)\n",
    "print(f\"Training accuracy: {accuracy_score(y_cv, svc_best.predict(X_cv))}\")\n",
    "print(f\"Hold out dataset test accuracy: {accuracy_score(y_finaltest, svc_best.predict(X_finaltest))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cba8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_finaltest, svc_best.predict(X_finaltest), labels=svc_best.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svc_best.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89816a32",
   "metadata": {},
   "source": [
    "#### Visualizing model coefficients\n",
    "\n",
    "Again inspecting the model coefficients can tell us about what features are important for the model to make its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel_coefs_df = pd.DataFrame({\"coef\": svc_best.coef_[0]})\n",
    "bestmodel_coefs_df[\"abs_coef\"] = bestmodel_coefs_df[\"coef\"].abs()\n",
    "bestmodel_coefs_df[\"coef_name\"] = iqms_scaled.columns\n",
    "bestmodel_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cf8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the coefficients by absolute value and plot them\n",
    "coefs_sorted_df = bestmodel_coefs_df.sort_values(by='abs_coef',ascending=False)\n",
    "#Stem plot\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.stem(\"coef\", data=coefs_sorted_df)\n",
    "plt.xticks(np.arange(len(coefs_sorted_df)), coefs_sorted_df[\"coef_name\"], rotation='vertical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "# Do not display warnings\n",
    "\n",
    "# Visualization in form of polar plot\n",
    "coefs_sorted_df[\"positive\"] = coefs_sorted_df[\"coef\"] > 0\n",
    "px.line_polar(coefs_sorted_df, r='abs_coef', theta='coef_name', color='positive', line_close=True, \n",
    "              hover_data={\"coef\": ':.3f', \"coef_name\": True, \"positive\": False, \"abs_coef\": False},\n",
    "              width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348e9470",
   "metadata": {},
   "source": [
    "Because the plot is too crowded to be readable let's define a threshold on the coefficient weight. \n",
    "Remember that SVC unlike Elastic-Net do not perform L1 regularization, that is why we end up with many more non-zero coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display \n",
    "# Do not display warnings\n",
    "\n",
    "# Visualize feature with highest weight. Visually we can see clusters with weight > 0.1\n",
    "coef_thr = 0.1\n",
    "px.line_polar(coefs_sorted_df[coefs_sorted_df[\"abs_coef\"]>coef_thr], r='abs_coef', theta='coef_name',\n",
    "              color='positive', line_close=True, width=800, height=600,\n",
    "              hover_data={\"coef\": ':.3f', \"coef_name\": True, \"positive\": False, \"abs_coef\": False},)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0aa2c6",
   "metadata": {},
   "source": [
    "We see that our results from the Elastic-Net classification holds and SNR-derived metrics remain the most important features to predict motion presence based on the IQMs. The other important metrics remain stable as well.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Both supervised classification methods give converging evidence that SNR-derived metrics are the most important features to classify motion strength based on the IQMs. Coming back to our original question of interpreting the IQMs, this result means that SNR-derived metrics are important for capturing motion artifacts. \n",
    "We have thus contributed to the understanding and interpretation of the IQMs. \n",
    "We hope that this notebook can now help you select the most important IQMs for your analysis or perform the analysis of your own IQMs and as such that you can help us further improve the interpretability of the IQMs generated by MRIQC.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This notebook has been initiated at the [Brainhack Global Geneva 2022](https://brainhack.ch) and springs from a joint effort of the project team composed by Mikkel Schöttner, Céline Provins, Michael Dayan, Vivi Nastase, Jenny Lunde and Oriol Mãne Benach. \n",
    "The results presented in this notebook have been submitted as an abstract to OHBM Montréal 2023. \n",
    "The abstract is entitled \"Signal-to-noise ratio estimates predict head motion presence in T1-weighted MRI\" and is available at [https://osf.io/7vqzr/](https://osf.io/7vqzr/). \n",
    "The full exploratory analysis we performed in the context of this project can be found at [https://github.com/brainhack-ch/interpret-iqms](https://github.com/brainhack-ch/interpret-iqms). \n",
    "The runnable notebook can be downloaded at [https://github.com/brainhack-ch/interpret-iqms/blob/main/code/interpretability-of-iqms.ipynb](https://github.com/brainhack-ch/interpret-iqms/blob/main/code/interpretability-of-iqms.ipynb).\n",
    "\n",
    "## References\n",
    "\n",
    "Nárai, Ádám, Petra Hermann, Tibor Auer, Péter Kemenczky, János Szalma, István    Homolya, Eszter Somogyi, Pál Vakli, Béla Weiss, and Zoltán Vidnyánszky. 2022. “Movement-Related Artefacts (MR-ART) Dataset of Matched Motion-Corrupted and Clean Structural MRI Brain Scans.” Scientific Data 9 (1): 630. https://doi.org/10.1038/s41597-022-01694-8.\n",
    "\n",
    "Zou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20. https://doi.org/10.1111/j.1467-9868.2005.00503.x."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "notebook_metadata_filter": "all,-language_info",
   "split_at_heading": true,
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": "0.8",
    "jupytext_version": "1.11.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "source_map": [
   15,
   27,
   48,
   68,
   78,
   83,
   97,
   106,
   111,
   118,
   120,
   135,
   137,
   142,
   144,
   150,
   154,
   159,
   163,
   172,
   177,
   195,
   206,
   214,
   219,
   225,
   240,
   249,
   253,
   270,
   291,
   311,
   313,
   324,
   330,
   338,
   343,
   351,
   353,
   366,
   372,
   381,
   383,
   390,
   398,
   402,
   409,
   414,
   419,
   428,
   433,
   442,
   454,
   456,
   461,
   469,
   475,
   477,
   484,
   491,
   495,
   502,
   510,
   520,
   529,
   537,
   546
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}